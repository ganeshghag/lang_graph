{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee547d1-6081-4761-aef0-900851190cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain\n",
    "# pip install langchain-ollama\n",
    "# pip install langgraph\n",
    "# ollama run llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a87a028-aeb2-46eb-9094-57f2712b1541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='What do you know about LangGraph?' additional_kwargs={} response_metadata={} id='802676c6-3cd3-4052-9775-0ae0e6559e52'\n",
      "{'messages': [HumanMessage(content='What do you know about LangGraph?', additional_kwargs={}, response_metadata={}, id='802676c6-3cd3-4052-9775-0ae0e6559e52'), HumanMessage(content='<think>\\nOkay, so I\\'m trying to figure out what LangGraph is. From the previous response, it sounds like it\\'s some kind of programming tool or framework for creating language models. Let me break this down step by step.\\n\\nFirst, I know that language models are used in AI to generate text based on input. For example, ChatGPT uses a form of these models. But how exactly does LangGraph work? The description mentioned it\\'s a library built with TypeScript and GPT-style training loops. So maybe it\\'s a framework or set of tools for developing custom language models.\\n\\nI remember that in machine learning, especially NLP tasks, attention mechanisms are important. The term \"self-attention\" comes up often in transformer models. LangGraph probably has these mechanisms because they\\'re crucial for processing sequences like text. It also mentioned training loops similar to GPT, which makes sense because GPT was one of the first models to use such loops for pre-training.\\n\\nThe fact that it\\'s built with TypeScript suggests that it\\'s not only a Python-based library but also uses other languages. TypeScript is often used alongside JavaScript and can simplify code writing by adding static type checking. So perhaps LangGraph leverages TypeScript for better code quality or error handling in the models.\\n\\nI\\'m curious about its modular architecture. The previous answer said it\\'s modular, which could mean that developers can plug in different components like optimizers, embeddings, etc., depending on their project needs. This would be helpful if someone wants to tweak a part of the model without affecting the entire structure.\\n\\nSpeaking of embeddings and other componentsâ€”word embeddings are another key part of NLP models. They convert words into vectors for easy computation. If LangGraph includes these, it can process input text more effectively by encoding words in meaningful numerical representations.\\n\\nPre-trained datasets seem to be a big part of training language models. Models like BERT use masked language modeling and skip-gram tasks. Maybe LangGraph uses similar pre-training datasets to allow users to train their models on existing data without having to collect all the data themselves, which can save time and resources for small projects.\\n\\nI wonder if LangGraph has a flexible input pipeline. Training large language models requires processing lots of text in an efficient way. If it works with JSONL files or text formats commonly used in NLP tasks, it might make training more straightforward for users.\\n\\nThe mention of extensibility is interesting. It means that the framework can be adapted to various specific use cases beyond general language modeling. So perhaps developers who have particular needs can add custom layers or modify parts of the architecture without needing a complete rewrite.\\n\\nIn terms of implementation details, using TypeScript with interfaces for model components makes sense because it helps in defining and working with complex data structures like tensors (arrays) needed in neural networks. This approach could make the codebase cleaner and more maintainable compared to other dynamically typed languages.\\n\\nIt\\'s also good that LangGraph is cross-platform since many projects work on multiple operating systems. Speaking of which, distributed training across GPUs sounds efficient if done right because those devices can process tasks in parallel, speeding up training times significantly.\\n\\nLastly, integrating with machine learning frameworks like TensorFlow or PyTorch suggests compatibility. This would allow users who are already familiar with these tools to expand LangGraph\\'s capabilities without switching entirely to a new ecosystem.\\n\\nSo putting it all together, LangGraph seems like a powerful and flexible library for building language models using transformer architectures. It combines modular design with TypeScript and GPU support, making it suitable for both experienced developers and maybe even those who are getting started in AI development.\\n</think>\\n\\n**LangGraph: An Overview**\\n\\nLangGraph is a cross-platform framework designed for constructing custom language models using the self-attention mechanism common in transformers like GPT. Here\\'s an organized summary of its features and structure:\\n\\n1. **Language Model Framework**: LangGraph serves as a library built with TypeScript, enabling the creation of advanced language processing models.\\n\\n2. **Self-Attention Mechanisms**: Incorporates key elements from transformer models to handle sequence data efficiently, crucial for tasks like text generation beyond static documents.\\n\\n3. **Modular Architecture**: The framework is designed to be flexible, allowing users to integrate various components such as optimizers and embeddings without altering the core structure.\\n\\n4. **Pre-Training Support**: Supports pre-trained datasets found in research models (e.g., BERT), facilitating quicker model training by leveraging existing datasets.\\n\\n5. **Efficient Training Pipeline**: Capable of handling large-scale data through JSONL or text formats, ensuring efficient processing with features suitable for distributed computing on GPUs.\\n\\n6. **Extensibility**: Designed to adapt to specific use cases, enabling developers to modify or extend the framework without extensive rewriting, tailored to their project needs.\\n\\n7. **Implementation Details**: Utilizes TypeScript with interfaces for components like tensors, promoting clarity and maintainability in code structure.\\n\\n8. **Cross-Platform Compatibility**: Supports multiple operating systems, ensuring broad applicability across different development environments.\\n\\n9. **Distributed Training Support**: Leverages GPU acceleration for faster training times by parallel processing tasks effectively.\\n\\n10. **Integration Capabilities**: Compatible with popular machine learning frameworks (TensorFlow, PyTorch), seamlessly integrating into existing ecosystems familiar to developers.\\n\\nOverall, LangGraph offers a robust and adaptable solution for building advanced language models, ideal for both seasoned developers looking to innovate and newcomers exploring AI development concepts.', additional_kwargs={}, response_metadata={}, id='126ebaff-0bd7-4f31-b9aa-6b026ce1613b')]}\n"
     ]
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "from openai import OpenAI\n",
    "llm = OpenAI(api_key=\"ollama\", base_url=\"http://127.0.0.1:11434/v1\")\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "def chatbot(state: State):\n",
    "    print(state[\"messages\"][0])\n",
    "    response = llm.chat.completions.create(\n",
    "    model=\"deepseek-r1:7b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\": state[\"messages\"][0].content}],\n",
    "    stream=False\n",
    "    )\n",
    "    \n",
    "    return {\"messages\": [HumanMessage(content=response.choices[0].message.content)]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "graph = graph_builder.compile()\n",
    "ret_message = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What do you know about LangGraph?\"}]})\n",
    "print(ret_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bad49f-0ecf-47dc-98af-6cb6a473ef6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2393fdf2-46eb-4fdd-af0e-dfabb3dca84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT WORKING in my env\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
